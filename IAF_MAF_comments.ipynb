{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"General normalizing flow set-up:\"\n",
    "\n",
    "> I think a \"general normalizing flow\" should be introduced in it's own section rather than within and IAF heading so that it's not confused/conflated with IAF specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{z}_0 \\sim q(\\mathbf{z}_0| \\mathbf{x}), \\mathbf{z}_t = f_t(\\mathbf{z}_{t-1}, \\mathbf{x})$\n",
    "\n",
    "> The role and notation of x in the first equation is not clear to me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Motivation of IAF:\"\n",
    "> I suspect planar/radial is not useful in high dimensions both from a computational perspective as well as in terms of its representational capability (in a similar way that SVMs cannot generalize as well as NNs in some settings), but I am not sure what you're intending to indicate with your statements in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $i>0$, $z^t_i = \\boldsymbol \\mu^t_{i}(\\mathbf{z}^{t-1}_{0:i-1}) + \\boldsymbol \\sigma^t_{i}(\\mathbf{z}^{t-1}_{0:i-1})*z^{t-1}_i$\n",
    "> $\\mu^t_{i}$ and $\\sigma^t_{i}$ should be bolded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a single forward pass $\\mathbf{y} = f_T\\circ \\dots \\circ f_1(\\mathbf{z})$ can be computed parallelly and efficiently, which benefits the sampling process.\n",
    "\n",
    "> For a given $t$, the parallelization is across $i$ in your $z_i^t$ notation; but, processing across $t$ must still be sequential. So, the statement that the composition \"can be computed parallelly and efficiently\" could more precisely be \"at each stage of the composition the univariate elements of transformation $f_t$ can thus be computed parallelly and efficiently\"\n",
    "\n",
    "> I think the efficiency considerations could more readily emerge if the writing was more oriented around illustrating this aspect of IAF.  From this it can become clear where IAF attempts to scale better than e.g., planar/radial flows: it is across the elements of $z_t$ which we may parallelize under this methodology, but the composition itself must still proceed sequentially.\n",
    "\n",
    "> The remainder of this section was must better in this regard; though, $\\mathbf{z}_{t-1}$ does not \"depend on its own (first $i - 1$) elements\", $z^{t-1}_i$ does (and so we may not parallelize the elements of z within each inverse transformation of the composition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Masked Autoregressive Flow (MAF)\"\n",
    "\n",
    "> I think that compared to the general $z_t = f_t(z_{t-1})$ notation, the specific notations \n",
    "- $z_t = f_{z_{t-1}}(z_{t-1})$ for IAF \n",
    "- $z_t = g_{z_t}(z_{t-1})$ for MAF \n",
    ">\n",
    "> are more useful since they show that \n",
    "- IAF forward elements $z_i^t$, given $z_{t-1}$, can be computed in parallel\n",
    "- whereas for MAF we can see that it is the elements $z_i^{t-1}$ of the inverse $z_{t-1} = g_{z_t}^{-1}(z_t)$ which can be computed in parallel given $z_{t}$.\n",
    "\n",
    "> The IAF inverse $z_{t-1} = f_{z_{t-1}}^{-1}(z_t)$ is the \"inverse\" of the MAF forward  $z_t = g_{z_t}(z_{t-1})$ in the sense that the positions of $z_t$ and $z_{t-1}$ are switched, and the same is true of the IAF forward relative to the MAF inverse. \n",
    "> - These role reversals, and the IAF moniker, follow directly from\n",
    ">  \n",
    ">   `bijector=tfb.Invert(tfb.MaskedAutoregressiveFlow(made)))`\n",
    ">\n",
    ">   which \n",
    ">   1. does not change `shift_and_log_scale_fn=made`\n",
    ">   2. but instead just swaps $f$ for $g^{-1}$ and $f^{-1}$ for $g$ so \n",
    ">\n",
    ">      IAF forward is now $z_t = f_{z_{t-1}}(z_{t-1}) = g^{-1}_{z_{t-1}}(z_{t-1})$ `# rather than MAF usage` $g_{z_t}^{-1}(z_t)$\n",
    ">       \n",
    ">      IAF inverse is now $z_{t-1} = f_{z_{t-1}}^{-1}(z_t) = g_{z_{t-1}}(z_t)$ `# rather than MAF usage` $g_{z_t}(z_{t-1})$\n",
    ">      <br>\n",
    ">\n",
    ">   3. which again notationally indicates the parallelizablity of the IAF foward pass (previously the MAF inverse pass)\n",
    "       - which is also seen in swapping the `forward` and `inverse` computations below:\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from\n",
    "# https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow\n",
    "\n",
    "def forward(x):\n",
    "  y = zeros_like(x)\n",
    "  event_size = x.shape[-event_dims:].num_elements()\n",
    "  for _ in range(event_size):\n",
    "    shift, log_scale = shift_and_log_scale_fn(y)\n",
    "    y = x * tf.exp(log_scale) + shift\n",
    "  return y\n",
    "\n",
    "def inverse(y):\n",
    "  shift, log_scale = shift_and_log_scale_fn(y)\n",
    "  return (y - shift) / tf.exp(log_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both IAF and MAF use MADE as their component layer. MAF is a normalizing flow that represents a invertable transformation applied to a distribution. MADE can be considered as a (network) layer, while MAF is a bijector with inverse operation. \n",
    "> IAF also is a bijector, and bijectors also have log det Jacobians for forward and inverse transformations.\n",
    "> \n",
    "> I think it's also worth noting that MADE layer outputs can define autoregressive probability distributions defining a joint distribution, e.g., of the form\n",
    ">\n",
    "> $$\\mathbf{z} \\sim N(\\boldsymbol \\mu(\\mathbf{z}), \\text{diag}(\\boldsymbol \\sigma(\\mathbf{z}))$$\n",
    ">\n",
    "> which is equivalent to the single bijection IAF defined in your \"estimate density using IAF\" section but\n",
    "> 1. can be sampled from directly\n",
    "> 2. can be evaluated directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFP implementation using MAF: 1) 2D example\n",
    "\n",
    "- You need `input_order='right-to-left'` so that you first model x2 and then x1|x2\n",
    "    - x2|x1 in the moon example is bimodal so not a nice natural way to condition\n",
    "- But then you also need a nonlinear transform so that \n",
    "    - x1 increases as x2<0 decreasing\n",
    "    - but also x1 increases for x2>0 increasing\n",
    "         - 'relu' is a usual choice for that\n",
    "    - and you need a good number of hidden units which combine their relu's to make the nonlinarity smooth\n",
    "        - e.g., `hidden_units=[50,50]` \n",
    "        \n",
    "Not sure they did any of this very well in the [TF docs](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow)...\n",
    "\n",
    "#### *But I like that you almost got your code working nonetheless!*\n",
    "- This below will do what you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "\n",
    "# Generate data X1 = X2^2 / 4\n",
    "\n",
    "n = 2000\n",
    "x2 = np.random.randn(n).astype(dtype=np.float32) * 2.\n",
    "x1 = np.random.randn(n).astype(dtype=np.float32) + (x2 * x2 / 4.)\n",
    "data = np.stack([x1, x2], axis=-1)\n",
    "plt.plot(x1, x2, \".\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer function of MADE (bijector function for MAF)\n",
    "made1 = tfb.AutoregressiveNetwork(params=2, hidden_units=[50,50], \n",
    "                                  activation='relu', input_order='right-to-left') # <- THIS\n",
    "\n",
    "# Target distrbution transformed by MAF (3 layers)\n",
    "distribution = tfd.TransformedDistribution(\n",
    "    distribution=tfd.MultivariateNormalDiag(loc=[0,0], scale_diag=[1,1]), # base: 2D Gaussian\n",
    "    bijector=tfb.MaskedAutoregressiveFlow(made1))\n",
    "# distribution = tfd.TransformedDistribution(distribution, bijector=tfb.MaskedAutoregressiveFlow(made2))\n",
    "# distribution = tfd.TransformedDistribution(distribution, bijector=tfb.MaskedAutoregressiveFlow(made3))\n",
    "\n",
    "# fit tf model\n",
    "inputs = tfkl.Input(shape=(2,))\n",
    "outputs = distribution.log_prob(inputs)\n",
    "model = tfk.Model(inputs, outputs)\n",
    "\n",
    "def neg_log(true, output):\n",
    "  return -output\n",
    "model.compile(optimizer=tf.optimizers.Adam(), loss=neg_log)\n",
    "model.fit(x=data, y=np.zeros(n), epochs=50, verbose=False) # y does not matter (unsupervised)\n",
    "\n",
    "# Sampling from the estimated density for visualization\n",
    "\n",
    "samples = distribution.sample(1000)\n",
    "x1_hat = samples[:, 0]\n",
    "x2_hat = samples[:, 1]\n",
    "plt.plot(x1_hat, x2_hat, \"r.\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "\n",
    "g = 100\n",
    "g1,g2 = np.meshgrid(np.linspace(x1_hat.numpy().min(), x1_hat.numpy().max(), g), \n",
    "                  np.linspace(x2_hat.numpy().min(), x2_hat.numpy().max(), g))\n",
    "grid = np.stack([g1.flatten(), g2.flatten()], axis=-1)\n",
    "plt.contour(g1, g2, distribution.prob(grid).numpy().reshape(g,g), \n",
    "            levels=np.logspace(-5,2,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MADE layer\n",
    "made = tfb.AutoregressiveNetwork(params=2, hidden_units=[512,512],\n",
    "                                 activation='relu', input_order='right-to-left') # <- THIS\n",
    "# IAF model \n",
    "iaf = istribution = tfd.TransformedDistribution(\n",
    "    distribution=tfd.MultivariateNormalDiag(loc=[0,0], scale_diag=[1,1]), # base: 2D Gaussian\n",
    "    bijector=tfb.Invert(tfb.MaskedAutoregressiveFlow(made)))  # just need to invert the maf\n",
    "# fit model\n",
    "inputs = tfkl.Input(shape=(2,))\n",
    "outputs = distribution.log_prob(inputs)\n",
    "model = tfk.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(), loss=neg_log)\n",
    "model.fit(x=data, y=np.zeros(n), epochs=50, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from the estimated density for visualization\n",
    "\n",
    "samples = distribution.sample(1000)\n",
    "x1_hat = samples[:, 0]\n",
    "x2_hat = samples[:, 1]\n",
    "plt.plot(x1_hat, x2_hat, \"r.\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "g = 100\n",
    "g1,g2 = np.meshgrid(np.linspace(x1_hat.numpy().min(), x1_hat.numpy().max(), g), \n",
    "                  np.linspace(x2_hat.numpy().min(), x2_hat.numpy().max(), g))\n",
    "grid = np.stack([g1.flatten(), g2.flatten()], axis=-1)\n",
    "plt.contour(g1, g2, distribution.prob(grid).numpy().reshape(g,g), \n",
    "            levels=np.logspace(-5,2,10))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
